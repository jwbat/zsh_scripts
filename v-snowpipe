#!/bin/zsh

echo "$sea
We’re experimenting with ingesting data from AWS IoT Core that arrives through MQTT and it’s really e
asy to hookup a rule to process each message and then do a variety of AWSy things with it, s
uch as: Put in S3 bucket, Call a Lambda, Kinesis, Kafka, etc.  I’m
wondering if you might have any time in the near future to help us out on getting it into Snowflake.
It’s timestamped IoT data just like what we have in Psql/Snowflake today, so at first I
was tempted to call a Lambda an insert it into our Psql DB which would then flow through to Snowflake b
y virtue of the nifty pipeline you created.  BUT…. each Lambda call would be opening a Psql connection a
nd killing it, unless we use the AWS DB Proxy which only works with RDS, so can’t quite use that yet.
So my next thought is that we would just shove each message into S3 and use Snowpipe to ingest the d
ata.  I think you’re familiar with this method, so you could let me know if there a
re any gotchas, such as: any problem with zillions of little files (one line CSV) on a
n S3?  Purging data after its successfully ingested into Snowflake? (does Snowpipe do that for us?).  How 
is the pricing model going to be for this ingestion?  Right now we only wakeup Snowflake periodically to batch f
rom Psql->SF.  Will we have to effectively have a WH always on to perform Snowpipe S3 ingestion?
$red
You are on the right track.  The Lambda seems like a bad idea because of how Snowflake billing works.
Each Lambda call would execute an insert, which requires a running Snowflake warehouse, which has a minimum 60 s
econd billable time each time it starts up.
Presumably you’d be getting messages fast enough that the warehouse would just never spin down, which means a
n XSMALL warehouse running 24/7.
Snowpipe on the other hand has a 1 second minimum bill time, so if you get a new record e
very other second then inserting with snowpipe would cost you half as much.
The number of files is potentially annoying, but easy to ignore.
Snowpipe just executes a COPY command (https://docs.snowflake.com/en/sql-reference/
sql/create-pipe.html) whenever it sees a new file, and that COPY command can delete files after successfully loading them.
Alternatively you could set a retention period on the S3 bucket if you wanted to keep, say..., a w
eek of files around.
Kafka is also a good option.  You can use the Kafka Snowflake Connector to stream messages from a kafka q
ueue into Snowflake in near real time.  The tradeoff is that Kafka has more moving parts and requires more k
nowledge to maintain than a simple S3 bucket solution.
In summary, you’re totally on the right track.  I’d ping Snowflake and ask which s
olution they recommend between S3 and Kafka.
"
exit 0
